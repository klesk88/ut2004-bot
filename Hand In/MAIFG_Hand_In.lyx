#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass paper
\begin_preamble
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{url} 
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref false
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title{Hand In for MAIfG 2012}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
author{{
\backslash
bf Michele Ermacora, Tilman Geishauser and Francesco Guerra}  
\backslash

\backslash
 {
\backslash
bf Games (Technology)} 
\backslash

\backslash
 {
\backslash
bf IT University Copenhagen}}
\end_layout

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Aim
\end_layout

\begin_layout Standard
Our main goal with this project was to experiment with techniques, to learn
 from implementing them and to have fun.
 We were interested in prediction with neural networks and GOAP.
 Whilst thinking about how to implement GOAP we decided that it would be
 interesting to use Monte Carlo Tree Search (MCTS in the following) instead
 of A*.
 Our agent architecture also uses GOAP as a starting point.
 
\end_layout

\begin_layout Standard
We would not use these solutions if we were writing actual Unreal bots as
 AI Programmers for Epic, and would have focused other techniques, for example
 mirroring of the opponents actions, if we were serious about participating
 in the 2K BotPrize.
\end_layout

\begin_layout Standard
A secondary goal was to create a bot that would be able to navigate the
 map in a reasonable fashion.
 Furthermore, in a competitive sense, the bot should be interesting to play
 against for the best Unreal player in the team, Michele, and at the same
 time the bot should be defeatable by the worst player, Tilman.
 
\end_layout

\begin_layout Section
Overall Design of the Bot
\end_layout

\begin_layout Subsection
Adaption of GOAP to Unreal 2004
\end_layout

\begin_layout Standard
Unreal 2004 is different from F.E.A.R.
 in many ways.
 Game play is less tactical and there are no cover and no smart objects
 in the environment.
 Champandard 
\begin_inset CommandInset citation
LatexCommand citet
key "Champandard_2007_FearAi"

\end_inset

 quotes Orkin on the matter which complexity GOAP is set out to solve: 
\end_layout

\begin_layout Quotation
“In F.E.A.R., A.I.
 use cover more tactically, coordinating with squad members to lay suppression
 fire while others advance.
 A.I.
 only leave cover when threatened, and blind fire if they have no better
 position.” 
\end_layout

\begin_layout Standard
Thus we found it difficult to translate the abstraction level Orkin uses
 for his actions to the simpler single player game play in Unreal.
 For instance weapon changing can be done instantly in Unreal, but is a
 major tactical element as cover or squad behaviors are less relevant.
 We thus decided not to solve weapon selection in the Target Manager as
 it is done in GOAP, but to let our planning system solve the weapon selection
 problem.
 For example the bot can retreat to a health point whilst firing minigun,
 flak cannon or assault rifles - all weapons which fire many bullets with
 spread and low accuracy.
 If the bot has high health he can however decide to kill the player by
 engaging him using the potentially more powerful shock gun nuke action.
\end_layout

\begin_layout Subsection
Setting and Performance
\end_layout

\begin_layout Standard
We only worked with 2-3 players at a time - usually testing with a 1-1 scenario
 and sometimes adding a second bot to test whether everything would work
 fine.
 This also reduced the significance of the working memory, for example target
 selection in such a simple scenario is not a highly interesting problem.
 Furthermore we do not need to care about detailed optimizations - we are
 using pogamut and gamebots, which is not an efficient solution compared
 to implementing bots inside a game, Unreal 2004 is an old game with low
 system requirements and we have only few agents at a time.
\end_layout

\begin_layout Subsection
Architecture comparison
\end_layout

\begin_layout Standard

\shape smallcaps
GOAP 
\shape default
uses an architecture - shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:GOAP-agent-architecture"

\end_inset

 - based on the C4 architecture by 
\begin_inset CommandInset citation
LatexCommand citet
key "Burke01creaturesmarts:"

\end_inset

.
 Sensors pass perceptions, including internal and external ones, to the
 working memory.
 The information in the working memory is used for planning once a goal
 has been selected - a key difference to the C4 architecture being the real
 time planner .
 If the facts in the working memory change the highest priority goal is
 re-evaluated.
 Subsystems such as navigation or the animation system are instructed via
 the blackboard, which stores information from these systems and the planning
 system.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Orkin2005"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename GOAP-architecture.png
	lyxscale 50
	scale 30

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:GOAP-agent-architecture"

\end_inset

GOAP agent architecture, taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "Long_MasterThesisGOAP"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Our architecture is similar.
 We did however not get to implement a proper working memory and use the
 blackboard instead.
\end_layout

\begin_layout Subsection
World state
\end_layout

\begin_layout Standard
Orkin uses fixed sized arrays for maintaining variables about a world state.
 Our world state is realized as a fixed size data structure by basing our
 world state boolean array on an enumeration of the symbols.
 As symbols in the goal state can be irrelevant in addition to being true
 or false, like uninstantiated variables in logic, we created an enum expressing
 true, false and uninstantiated.
 A third enumeration describes the possible goal, and a switch case in the
 world state class can safely create goal states based on this enumeration.
\end_layout

\begin_layout Itemize
Goals: KillEnemy, SearchRandomly, EmptyAmmunition, Survive, SearchAdrenaline,
 SuppressionFire, FindWeapons
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
In the appendix (orkin 2005 paper) 17 world state symbols are used.
 We already have 14 symbols, and many are expressed with the two symbols
 WeaponArmed and WeaponLoaded in GOAP.
 If we were to create tactical depth comparable to GOAP we could easly grow
 our world state description to a size much higher than the one used in
 GOAP.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Actions
\end_layout

\begin_layout Standard
We implemented various actions that can be used by the planner.
 
\end_layout

\begin_layout Itemize
Random Walk: Randomly run around on the map, trying to find a player.
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions
\end_layout

\end_deeper
\begin_layout Itemize
Retreat with suppression fire: Try to find a health pack, and shoot at the
 player with minigun, flak cannon or assault rifle.
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions:
\end_layout

\end_deeper
\begin_layout Itemize
Find shock gun ammuntion: If there is shock gun ammunition on the map go
 there.
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions:
\end_layout

\end_deeper
\begin_layout Itemize
Shock gun nuke: Fire the secondary mode of the shock gun, and after some
 time shoot the primary mode.
 If the primary mode hits the secondary projectile a huge explosion will
 happen.
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions:
\end_layout

\end_deeper
\begin_layout Itemize
Find health pack: Look for health packs.
 Which health pack to go to depends on various factors (TODO: FINISH THIS
 TEXT)
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions:
\end_layout

\end_deeper
\begin_layout Itemize
Shoot grenade: Will fire a grenade at a player position.
\end_layout

\begin_deeper
\begin_layout Itemize
preconditions:
\end_layout

\begin_layout Itemize
postconditions:
\end_layout

\end_deeper
\begin_layout Section
Planner
\end_layout

\begin_layout Standard
The planning is done inside the GOAPPlanner class and replanning is triggered
 every time the state of the world changes in a way that makes a new goal
 more desireable than the current one, or when the actual plan either fails
 or succeeds.
 To create a plan our system uses Monte Carlo Tree Search (MCTS) with the
 UCT formula (we use as a starting point Michele's MCTS used for the PacMan
 project).
 We decided to use this algorithm, instead of the usual A* used by Orkin,
 for different reasons.
 On the one hand we were just interested in trying this out.
 We were furthermore expecting that we can search a more vast state space,
 while with the A* algorithm we take in each step the best action possible
 in that moment and then we iterate on that.
 With the MCTS we perform also exploration of other nodes that can lead
 to other interesting plans - we were hoping to achieve more variety in
 agent behaviour.
 For example we could find two different plans which satisfy the same goal
 and have the same reward, and a different one randomly every time.
 Although, while the MCTS is probably more computational expensive in tems
 of CPU and RAM usage, it gives a desirable degree of control in the sense
 that the user can dynamically change the time used by the algorithm to
 create a plan and the number of simulations inside the simulation step.
 This allows us to scale the artificial intelligence of the bot to powerful
 machines.
\end_layout

\begin_layout Standard
The MCTS takes as parameters the actual goal and world state.
 From these it tries to look for a plan.
 
\end_layout

\begin_layout Standard
The most significant changes inside the algorithm are inside the Node class,
 as it is in this class that, for each node created, we store the new informatio
n regarding action selected, new world state, new goal state.
 The Node class also performs the simulation step.
 The algorithm works as follow:
\end_layout

\begin_layout Itemize
The MCTS applies the tree policy and, if the node has not yet generated
 all his children, it creates a new node passing, as parameters, the goals
 state and the world state simulated by the parent from which the node was
 created plus the action selected for the child.
 The action selection is done by the node, before creating the new child,
 and is picked from a pool of actions that represent the ones that can,
 by applying the postconditions, satisfy in a partial or complete way the
 goal state.
 In case the node is already fully expanded it picks the child selected
 by the UCT formula and tries to expand it as in the standard algorithm.
 
\end_layout

\begin_layout Itemize
After the node is created, it updates its internal goal state and world
 state by applying, in order (SEE EDMUND LONG p 33): 
\end_layout

\begin_deeper
\begin_layout Itemize
To a temporary copy of the world state the post condition of the action
 that was chosen for this node
\end_layout

\begin_layout Itemize
To a temporary copy of the goal state the pre conditions of the action choose
 for this node
\end_layout

\end_deeper
\begin_layout Itemize
Then the MCTS runs the simulation step where it has to calculate the delta
 value for that node.
 In the simulation step we give a big reward if the action chosen is a terminal
 condition - that is, it is a goal state.
 A small reward is given if one of the action has to take further steps
 to reach a final condition.
 This value is a product of 1 divided be the count of unsatisfied preconditions
 and an designed reward that allows us to control the value of that particular
 action according to our design decisions.
\end_layout

\begin_layout Itemize
Finally, in the back propagation step, we back propagate the values.
 While also updating the parent's node best child variable in case the current
 node is better than the current one.
 This variable is used later to access in a fast way, from the root, the
 different actions composing the plan.
\end_layout

\begin_layout Standard
When the planner finally receives a plan from the MCTS, it stores it in
 the form of a stack in the blackboard to be consumed by the actor system.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename long_plan.png
	lyxscale 40
	scale 16

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Application of actions during planning, taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "Long_MasterThesisGOAP"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Designed rewards
\end_layout

\begin_layout Standard
We gave every action a value that expresses how much we, as designers of
 the system, would like to see the bot do the action.
 For example shooting the rocket launcher has a high reward, whereas searching
 rocket launcher ammunition has a low reward.
 These values, whilst using a hardcoded base value, are calculated each
 time the bot is updated.
 For example if we are already have ammunition for the rocket launcher finding
 more ammunition of that kind is less attractive.
\end_layout

\begin_layout Section
Movement
\end_layout

\begin_layout Standard
The low level logic of the movement is handle by the MovementLogic class.
 We have three different kind of behaviours which are controlled by our
 actions:
\end_layout

\begin_layout Itemize
If the bot can see an opponent and is shooting at him, the HuntingPlayer
 behaviour is activated.
 In this context, the bot tries to reach the player while performing random
 movements (as strafe left, right or double jump) if it is hit or it can
 see a projectile coming.
\end_layout

\begin_layout Itemize
A random walk behaviour, that make the bot move by taking random navigation
 points of the map.
 This behaviour is triggered when the bot is looking for other players and
 as fall back behaviour.
\end_layout

\begin_layout Itemize
A specific behaviour, in which the bot tries to reach a well defined point
 in the map selected by some of our actions.
 For example if the bot is looking for health he will go to health pack
 preferring, in the following order spawned, visible and strong health packs.
\end_layout

\begin_layout Standard
These methods are used only if there is no collision.
 If our raycast system, composed out 5 rays [COMPARISON WITH PAPER], detects
 collisions we move the bot manually and try to make it unstuck.
 This system doesn't work very well because we do not remember problematic
 areas, neither do we check whether a planned movement will get the bot
 stuck.
 Thus, if the bot is actually engaged in a fight, even if it gets unstuck,
 immediately thereafter the bot can perform a random movement that get him
 stuck again.
\end_layout

\begin_layout Section
Prediction of enemy position with a neural network
\end_layout

\begin_layout Standard
One of the human features that for example hunters exhibit is to predict
 the location of some object after observing its previous movements.
 When an hunter, or, inour case, a player that is playing a first person
 shooter, has to predict the location of an object in the future in order
 to shoot at it, he considers some variables like the previous locations
 of the object, the speed of the object (and also his speed if he is moving),
 the speed of the projectile and the distance between him and the object.
 This human feature is not infallible, in fact if his assumptions are not
 met, for example when the object makes an unexpected movement, the hunter
 will miss the object.
 
\end_layout

\begin_layout Standard
We have tried to model this human feature and we modeled this behavior with
 a neural network.
 Between all the artificial intelligence techniques that we have studied
 during the lectures, we thought that the neural network is the best technique
 for this kind of problem, since the purpose is to understand a quite complex
 function that involves many variables.
 It is not the optimal solution for this problem, for example it is possible
 to use the prediction calculated by a Kalman filter 
\begin_inset CommandInset citation
LatexCommand cite
key "Welch:1995:IKF:897831"

\end_inset

 by setting the error in the matrix to zero.
 We were however not interested in an optimal solution, but in a solution
 that helps creating a fun, interesting and human like interaction with
 the bot.
 We also decided that it would be interesting to experiment with neural
 networks in a concrete and quite complex problem: a First Person Shooter
 environment.
 
\end_layout

\begin_layout Subsection
Input and Postprocessing of Output
\end_layout

\begin_layout Standard
First, we had to decide which input to use.
 Basically we had to decide whether to collect a huge amount of player behaviour
s, like moving really fast on left and right, walking around, jumping continuous
ly, and so on.
 This way we could have worked on long-term prediction.
 On the other hand we could collect simple, almost linear, movement, so
 that the neural network was able to infer a position in the near future,
 based on simple consideration.
 We thought at the end that the best solution would have been just to have
 short time prediction for two reasons: First it is more similar to human
 hunting behavior and exhibits the same shortcomings; Second it’s easier
 to create a training set for the neural network to have better results.
 
\end_layout

\begin_layout Standard
Another problem very important that emerged was what kind of inputs we had
 to use for the neural network.
 The output had to be a position, which means three different outputs representi
ng the position on X, Y and Z.
 Analyzing the Pogamut framework we realized that we didn’t have to use
 the position of our bot as input, since there is a function allowing to
 shoot at a desired position.
 Therefore only focused on the parameters of the enemy.
 Additionally we assumed that we needed the previous position of the enemy
 and the corresponding time at which each position was recorded.
 Because of the fixed updates of the bot logic we receive the position of
 the enemy at regular intervals.
 Then we thought that if the time-frame which we take the position of the
 enemy is regular, we didn’t need time as input.
 Since in this way the time is a constant, the neural network doesn’t need
 it anymore to build the function.
 Another advantage of not using time as input is that the Pogamut Framework
 and the multi-threaded nature of the environment we work in results in
 our code being executed at slightly different times.
 We measured this and the error is usually in the range of some milliseconds.
 As the enemy movement logic is executed at the same intervals as our bot
 logic, using exact times of retrieval of bot position as input might have
 introduced small errors.
 
\end_layout

\begin_layout Standard
For our first test we decided to not use the direction and the speed of
 the enemy, because we thought that the neural network could be able to
 “infer” those parameters, and at the same time they could have been misleading
 for the neural network (even though we don’t have proof of that because
 we didn’t implement that solution since the neural network worked in the
 first test).
 One issue we had was to decide how many predicted position we needed to
 have and at which frame interval.
 This problems come from the fact that each weapon has a different speed,
 and the time which the projectile reaches the target, also change according
 to the distance from it.
 To solve this problem we have chosen to have only one mid-term time prediction,
 and retrieving the all the predictions in between time 0 and the predicted
 time of the neural network, making a linear interpolation with vectors.
 In order to apply that algorithm we assumed that the movement of the enemy
 is linear.
 So we develop a lerp function that takes as parameter two locations and
 a weight.
 The first parameter is the current enemy position, the second one is the
 predicted position from the neural network and the third is a weight between
 0 and 1, where 0 represent the first position, 1 the second one and all
 the number in between represent a linear interpolation between the two
 vectors.
 To decide the best setting for the neural network we had to face some problem.
 The first problem came from the Pogamut framework (was actually inherited
 from Unreal).
 That was not actually a problem, was more a limitation to avoid unfair
 bot behaviors.
 Basically is not possible to get an enemy position if it is not visible.
 In this way was not possible to make an instant prediction since we needed
 to collect previous locations to set the inputs for the neural network.
 The second problem was setting a proper value for the output of the neural
 network, which means a prediction in one particular time of the future.
 To set the best value we look at the speed of several weapons and we have
 noticed that the slower one was the rocket launcher.
 So we tested the speed of the projectile of the rocket launcher from several
 distance and we found a trade-off.
 So basically the prediction was stetted to 0.6 second.
 Having all those data we could also find a proper setting for the input.
 So we assumed a good setting for the neural network was 4 different inputs,
 each one a position whit 0.2 second of difference and the output a position
 in the future, 0.6 second of different from the last input.
 In implementing the neural network we also faced other problem like the
 preparation and the normalization of the data (inputs and the desired output
 for the back propagation in the training phase).
 The first option was to give as input the coordinate in world frame or
 local frame relative to the bot.
 Neither of those solutions was good, essentially for the problem of normalizati
on.
 Each level is different and has a set of minimum and maximum coordinate,
 in this way is impossible to find a value that fits, in a proper way, our
 problem.
 So we came out with the idea of setting the input in local coordinate relative
 to the first input (position) in the neural network.
 The normalization factor was 400 which mean the max number of units a bot
 can cover in 0.6 second and the output was the position relative to the
 last input of the neural network.
 At that point the setting of the neural network was complete, with 12 input
 (4 positions) and 3 outputs (1 position).
 Basically the bot needs 0.8 second from when it sees a player to begin to
 make a prediction.
 If it visually lost its target for more than 0.1 second it has to start
 to collect data from scratch.
 To solve this problem the class Predictor, which manage data and the neural
 network to perform the prediction, returns the current target location
 in case a prediction is not available.
\end_layout

\begin_layout Subsection
Training Phase
\end_layout

\begin_layout Standard
We decided to create the data set to train the neural network with back
 propagation, recording the position of a character we were playing with.
 Since the Pogamut framework doesn’t allow getting the position of a player
 if it is not visible, we had to implement a UT Server that is omniscient,
 in order to get our character position for each frame.
 In the beginning we have thought that there was not possible to create
 incoherent and misleading data for the neural network just moving around
 the player, but it was wrong and we are going to explain it in detail after.
 After we collected the inputs from several games, we had to prepare and
 convert the data for the neural network, which means convert it from world
 to local frame, normalize and put it in proper data structure so that the
 neural network was able to read it (square arrays).
 The first test didn’t work.
 We have tried various topologies of the neural network and several values
 for the learning rate, but both the average error (calculated as desired
 output – actual output) for all the samples and the error of some of the
 sample was extremely slow (almost null) in converging.
 The only thing the neural network was able to understand was that when
 the target doesn’t move in the previous position, it will also not move
 in the future.
 Basically the most of the output were near to 0, which means that the player
 is not moving from the last input position.
 After a while we understand the reason why: the data we collect were incoherent.
 Basically moving randomly we generated incoherent data.
 For example in one case the inputs were positive positions along a direction
 and the output (the position after 0.6 second after the last input) was
 also a positive value, the second time with the same kind of input we had
 a negative output, which means that we changed direction in the end of
 the path.
 After we understood that error, we tried to generate training data again,
 trying to not make movement that could have been misleading for the neural
 network.
 So basically we moved our character almost in a linear way, trying to cover
 as much direction was possible.
 Then we tested this new training set and we saw that it was working since
 the average error was decreasing, so data were converging.
 Then we have tried several experiments to optimize the result.
 Unfortunately the data that we have collected are not trusted since for
 each setting of the neural network the convergence speed was pretty low.
 So from the data that we had we choose a setting that gave us a better
 speed of convergence more than an accurate result, even because we didn’t
 need a very high accuracy for our bot.
 So we have chosen a learning rate of 0.9 (very high) and a topology following
 the “rule of thumb” [1], setting the number of the hidden layer as the
 average between the input and the output layer.
 When we put the neural network into the project we only had to perform
 the forward method and get the output.
 The only problem was to give a meaning to the data, which means to apply
 the inverse operation of normalization and conversion from local to world
 frame.
 When we tested the result we discovered that the neural network was not
 able to make a right prediction for all the directions.
 This is the reason why has been a big mistake producing the training set
 “by hand”.
 If we had more time, an optimal solution to solve this problem would have
 been to program a bot that moves with different speed along a vector and
 then covers all the possible direction (basically means rotate the direction
 vector a bit), just to be sure to cover enough directions so that the neural
 network is able to build a proper function.
\end_layout

\begin_layout Standard
[1] http://www.heatonresearch.com/articles/5/page2.html rule of thumb 
\end_layout

\begin_layout Section
Evaluation
\end_layout

\begin_layout Standard
Interaction with the bot is enjoyable for a short time.
 He can do some fun things, for example he gets the shock gun nuke right
 at times, or dodges enemy fire interestingly when retreating.
 At the same time the bot is not challenging enough to create a really fun
 engagement.
 
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Movment.
 A way for resolve this bug could be check, before perform a random movement,
 if the bot will collide with some other objects.
\end_layout

\begin_layout Subsubsection*
\begin_inset Note Note
status collapsed

\begin_layout Subsubsection*
CERA-CRANIUM architecture
\end_layout

\begin_layout Paragraph*
2K BotPrize
\end_layout

\begin_layout Plain Layout
The 2K BotPrize is an annual contest in which the task is to create a computer
 game bot which is indistinguishable from a human player.
 Testbed for the competition is the first person shooter Unreal Tournament
 2004.
 Human players and bots play against each other and are judged on their
 'humanness'.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Note Note
status open

\begin_layout Plain Layout
introduce footnote by linking to 'humaness'
\end_layout

\end_inset

In 2010 a new automated judging system was used which we found interesting:
 A weapon called Link Gun can be used.
 If this is fired on a player and this player is human, the opponent will
 die and the attacker gets 10 kills instead of 1.
 If the victim of the gun is however a bot, then the attacker will die from
 the attack and lose 10 kills.
 There is a similar gun for attacking bots, rewarding the player with 10
 kills if the gun is used to hit a bot and vice versa.
 Instead of the score of the games, which is constituted by the amount of
 kills, the BotPrize is awarded to the bot which has the best ratio of being
 judged as human instead of bot.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
CITE WEBPAGE
\end_layout

\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
We do not know the full data set, but unless the Link Gun was used by every
 player on every player it might also be interesting to consider the total
 amount of how often a bot was judged as human and how often as bot, since
 if a player is uncertain about the nature of another player he might not
 use the Link Gun, but confidence about the bot or human nature of other
 players might be an interesting criterion.
 
\end_layout

\end_inset


\end_layout

\end_inset

 A question asked on the front page of the 2010 homepage is 
\begin_inset Quotes eld
\end_inset

Can a computer be programmed to seem to have personality, fallibility and
 cunning?
\begin_inset Quotes erd
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
CITE!!
\end_layout

\end_inset

 Apparently personality is considered amongst the main criteria, 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Are there any remarks on personality in any of the papers by the participants?
\end_layout

\end_inset

.
\end_layout

\begin_layout Paragraph*
The CERA-CRANIUM Architecture
\end_layout

\begin_layout Plain Layout
In 2010 the winning bot was Conscious-Robots, a bot based on the CERA-CRANIUM
 architecture by the team from the Computer Science Department, Carlos III
 University of Madrid.
  
\end_layout

\end_inset

 
\end_layout

\begin_layout Subsubsection*
\begin_inset Note Note
status collapsed

\begin_layout Section
Planners in Computer Game AI
\end_layout

\begin_layout Plain Layout
The first person shooter F.E.A.R.
 introduced planning systems to game A.I.
 in the year 2005 and this being written is ranked as second most influential
 game A.I.
 by aigamedev.com 
\begin_inset CommandInset citation
LatexCommand citep
key "aigamedev.comTOP10"

\end_inset

.
 The planning system in F.E.A.R., called Goal Oriented Action Planning (GOAP
 in the following), is a game specific adaption of the academic planning
 system STRIPS
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
STRIPS stands for STanford Research Institute Problem Solver
\end_layout

\end_inset

 by 
\begin_inset CommandInset citation
LatexCommand citet
key "fikes71strips"

\end_inset

.
 Instead of STRIPS we introduce an equivalent planning approach in propositional
 notation.
 We then discuss how GOAP differs from propositional planning, followed
 by discussions of advantages and applicability of GOAP.
\end_layout

\begin_layout Subsection
Propositional Planning
\end_layout

\begin_layout Plain Layout
We have chosen to introduce planning in propositional notation as we think
 that this notation allows to get a precise and comparably approachable
 idea of the concept of planning.
 It will help to stress features of GOAP that are special and not general
 in planning.
\end_layout

\begin_layout Plain Layout
Planning is searching a sequence of actions to satisfy a goal.
 A goal is some desired state of the world - a state is a description of
 the world rather than a procedural state as in the context of FSMs.
 Each action can only be chosen by the planner when its preconditions are
 met and executing the action will have effects on the state of the world.
 A plan, the thing we want to produce, then is the ordered list of actions
 that will transform the given state into the state that is desired.
\end_layout

\begin_layout Plain Layout
Our notation of propositional planning is based on the adaption by 
\begin_inset CommandInset citation
LatexCommand citet
key "bylander1991"

\end_inset

 of the notation introduced by 
\begin_inset CommandInset citation
LatexCommand citet
key "DeanBoddy1988"

\end_inset

.
 Before formulating an algorithm which will produce a plan we need a language
 which will represent states, goals and actions.
 The language of a propositional planer is a 4-tuple 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $(P,O,I,G)$
\end_inset

:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P$
\end_inset

 is a set of conditions, these are propositional variables (they are either
 true or false).
 
\end_layout

\begin_layout Itemize
\begin_inset Formula $O$
\end_inset

 is a set of operators, each operator being a quadruple 
\begin_inset Formula $O=(\alpha,\beta,\gamma,\delta)$
\end_inset

 with:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha\subseteq P$
\end_inset

 is a set of positive preconditions - conditions that have to be true for
 the postconditions to take effect.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\beta\subseteq P$
\end_inset

 is a set of negative preconditions - conditions that have to be false for
 the postconditions to take effect.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\gamma\subseteq P$
\end_inset

 is a set of positive postconditions - conditions that are made true by
 the operator.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\delta\subseteq P$
\end_inset

 is a set of negative postconditions - conditions that are made false by
 the operator.
\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $I$
\end_inset

 is a set of elements of 
\begin_inset Formula $P$
\end_inset

 which are true initially, all other elements of 
\begin_inset Formula $P$
\end_inset

 are assumed to be false.
\end_layout

\begin_layout Itemize
\begin_inset Formula $G$
\end_inset

 is a pair 
\begin_inset Formula $\left(N,M\right)$
\end_inset

 with 
\begin_inset Formula $N$
\end_inset

 denoting the elements which should be true and 
\begin_inset Formula $M$
\end_inset

 denoting the elements of 
\begin_inset Formula $P$
\end_inset

 which should be false, this is the goal state.
\end_layout

\begin_layout Plain Layout
Operators can be applied to any state 
\begin_inset Formula $S\subseteq P$
\end_inset

.
 If 
\begin_inset Formula $\alpha\subseteq S$
\end_inset

 and 
\begin_inset Formula $\beta\cap S=\emptyset$
\end_inset

 the new state is 
\begin_inset Formula $S\cup\gamma$
\end_inset

 with all conditions in 
\begin_inset Formula $\delta$
\end_inset

 set to false, otherwise the new state is the same as the old state.
 A plan then is the sequence of operators 
\begin_inset Formula $(o_{1},o_{2},...,o_{n})$
\end_inset

 which transforms the initial state 
\begin_inset Formula $I$
\end_inset

 to the goal state 
\begin_inset Formula $G$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "bylander1991"

\end_inset

 shows that planning quickly becomes an intractable problem.
 The complexity of a planning problem is caused by the number of pre- and
 postconditions.
 Those are, however, theoretical results for general planning problems and
 by using heuristics and other means efficient planning can be implemented,
 as demonstrated for example by GOAP.
\end_layout

\begin_layout Subsection
GOAP as Demonstrated by F.E.A.R.
\end_layout

\begin_layout Plain Layout
The term GOAP is coined by Jeff Orkin, who also is the mind behind this
 specific planning approach.
 GOAP has been developed specifically for computer games and allows efficient
 real time planning.
 It was developed for the first person shooter F.E.A.R.
 and has been used in a variety of other games 
\begin_inset CommandInset citation
LatexCommand citep
key "OrkinGOAPWebpage"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This website also offers a variety of useful material on GOAP including
 source code and articles discussing GOAP.
\end_layout

\end_inset

.
 One particularly interesting part of the motivation to use planning is
 mentioned in 
\begin_inset CommandInset citation
LatexCommand citet
key "orkin2006"

\end_inset

 
\begin_inset Quotes eld
\end_inset

[...] we had only one A.I.
 programmer, but there are lots of A.I.
 characters.
 The thought was that if we can delegate some of the workload to these A.I.
 guys, we'd be in good shape.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Plain Layout
What was called set of conditions in propositional planning is referred
 to as state in F.E.A.R..
 A state in F.E.A.R.
 is an assignment to some set of variables, but it is noteworthy that the
 data types of the variables can differ, so a state might be described partially
 by booleans, partially by integers, partially by further data types 
\begin_inset CommandInset citation
LatexCommand citep
key "Orkin2005"

\end_inset

.
 One example by 
\begin_inset CommandInset citation
LatexCommand citet
key "orkin2006"

\end_inset

 for describing the currently sunny weather of a world would be using a
 state variable named weather to which one value from a set of enumerated
 discrete values (rainy, sunny) is assigned, namely sunny.
 
\end_layout

\begin_layout Paragraph*
Operators in GOAP
\end_layout

\begin_layout Plain Layout
In a number of ways operators differ from propositional planning and STRIPS,
 this is discussed in detail in 
\begin_inset CommandInset citation
LatexCommand citet
key "orkin2006"

\end_inset

 and briefly in the following.
 
\end_layout

\begin_layout Plain Layout
In propositional planning each operator had a set 
\begin_inset Formula $\gamma$
\end_inset

 of positive postconditions and a set 
\begin_inset Formula $\delta$
\end_inset

 of negative postconditions.
 In STRIPS this would be achieved by two lists ordering all conditions that
 should be set to true or set to false respectively.
 GOAP however chooses a fixed sized array storing preconditions and the
 postconditions as key-value pair.
 This makes it easy to look up how a goal or precondition can be achieved
 as Orkin illustrates with the attack and the reload actions, having 'weapon
 is loaded' as precondition in the first case and as postcondition in the
 reload case.
 The fixed sized array comes with its own limitations, which are circumvented
 by deciding some things outside the planner, for example one of three weapons
 is selected by a subsystem and the planner just solves problems arising
 with weapons in general (e.g.
 the reloading as mentioned).
 
\end_layout

\begin_layout Plain Layout
Furthermore procedural pre- and postconditions are used.
 The case of procedural preconditions is illustrated by the behavior of
 escaping.
 In order to escape it would be preferable to run away, but as precondition
 the search for a safe path has to come up with a valid result, and such
 a search is only done when running away is considered.
 This is cheaper than always keeping track of whether a safe escape route
 is existent in the world description.
 Procedural postconditions are used, as some actions, like running away,
 should take some time and not be finished immediately.
 Fleeing is mentioned as an example, as its postcondition will give the
 orders necessary to move the NPC to a destination over some time (only
 after arriving at this destination a consequent action should be executed).
 
\end_layout

\begin_layout Plain Layout
Unlike operators defined in STRIPS or propositional planning, GOAP operators
 have costs, for example fleeing might have a cost of 3 and crouching a
 cost of 7, thus fleeing would be preferred.
 This allows to search the graph consisting of the different sequences of
 operators with an informed search algorithm.
 GOAP uses A* 
\begin_inset CommandInset citation
LatexCommand citep
key "AStarOriginal"

\end_inset

, a breadth first search algorithm evaluating each path by assigning a value.
 This value is the sum of a heuristic, estimating the distance to the goal,
 and the cost information, which represents the costs already accumulated.
\end_layout

\begin_layout Subsubsection*
GOAP is Efficient Enough for Computer Game AI
\end_layout

\begin_layout Plain Layout
As mentioned in our discussion of Bylander's analysis of propositional planning
 complexity can be an issue in planning.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Orkin2005"

\end_inset

 mentions that in order to make planning feasible in the domain of computer
 games, GOAP minimizes search iterations and keeps evaluation of preconditions
 light weight - processing of preconditions is distributed over many frames
 and results for the planner are cached and can be queried on demand.
 The previously mentioned fixed-sized array has a key role speeding up the
 planning process.
 
\end_layout

\begin_layout Plain Layout
Eric Jacopin, defending GOAP against the considerable worries of game developers
 concerning the efficiency and usefulness of planners mentioned in the panel
 discussion 
\begin_inset Quotes eld
\end_inset

The Future of Deliberative Decision Making
\begin_inset Quotes erd
\end_inset

 at the Paris Game AI Conference 2011, that in 65% of the cases seen in
 F.E.A.R.
 a plan would only have 2 actions.
\end_layout

\begin_layout Plain Layout
Due to the adjustments made to planning by Orkin, and as has been proven
 by the use of GOAP in a number of games, GOAP is efficient enough to be
 used in today's real time computer games.
\end_layout

\begin_layout Subsubsection*
Advantages of GOAP
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "orkin2006"

\end_inset

 mentions three main advantages GOAP has in comparison to FSM:
\end_layout

\begin_layout Paragraph*

\series bold
Modularity of Goals and Actions:
\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename orkin_action_sets.png
	width 69page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Three-different-action"

\end_inset

Three different action sets in F.E.A.R.
 Picture taken from 
\begin_inset CommandInset citation
LatexCommand citet
key "orkin2006"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
This decoupling of goals and actions allows addition of new characters to
 the game quickly.
 Especially if the character reuses actions other NPCs use as well, action
 sets as shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Three-different-action"

\end_inset

 can be put together easily and don't pose a threat to the stability of
 other NPCs.
 Furthermore goals are not blackboxes as they were in earlier games created
 by Monolith, the company behind F.E.A.R..
 This circumstance is illustrated by Orkin with the case of a NPC controlled
 by a FSM being shot whilst following the goal to finish work at the desk.
 The NPC would then finish the work at the desk so that the respective goal
 is achieved before becoming aware of the next goal, which would be to fall
 on the floor and be dead.
 
\end_layout

\begin_layout Paragraph*

\series bold
Transitions Between Behaviors are Created Procedurally:
\series default
 
\end_layout

\begin_layout Plain Layout
Or, as Richard Bull, who worked on a strategy game called Empires:Total
 War, illustrates, planning is 
\begin_inset Quotes eld
\end_inset

like a state machine that re-writes itself
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand citep
key "HowAIinGamesWork09"

\end_inset

.
 Orkin illustrates this with the example of adding late in development the
 behavior that characters should turn on the light when entering a room.
 Using a FSM this was a complicated task as changes everywhere in the FSM
 were necessary, whereas in a planning system all that would have been needed
 is an operator with the postcondition that light is turned on, and the
 preconditions of the 
\begin_inset Formula $GoTo$
\end_inset

 operator being extended by a 
\begin_inset Formula $LightOn$
\end_inset

 precondition.
\end_layout

\begin_layout Paragraph*

\series bold
Problems are Solved Dynamically:
\series default
 
\end_layout

\begin_layout Plain Layout
Whilst the first two advantages make implementation easier, GOAP really
 shines when goal-oriented and dynamic problem solving is favored as characteris
tic for the behavior of the NPCs.
 Orkin describes a scenario in which the player blocks a door with his body:
 
\begin_inset Quotes eld
\end_inset

[...]we will see the A.I.
 try to open the door and fail.
 He then re-plans and decides to kick the door.
 When this fails, he re-plans again and decides to dive through the window
 and ends up close enough to use a melee attack!
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "orkin2006"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Unpredictability is Better than Randomness
\end_layout

\begin_layout Plain Layout
A game providing an open-world experience we found quite interesting is
 Fallout 3, which is also using a planning approach in fight situations
 
\begin_inset CommandInset citation
LatexCommand citep
key "HowAIinGamesWork09"

\end_inset

.
 According to our game play experience the procedurally created encounters
 in many corners of the large game world are challenging and interesting.
 Situations which are not predictable by the player emerge when re-planning
 and events in the game world come together.
 Personally we spend roughly 100 hours in the game world provided by Fallout
 3 and others have played this game even longer.
 Scripting encounters for such a long game time does not seem approachable.
 Using FSMs has downsides discussed earlier.
 Due to the dynamic problem solving capacity of GOAP unpredictable situations
 are frequent.
 Unlike in rule based systems or FSMs unpredictability is not created by
 a careful use of randomness, but by artificial intelligence technology.
 As mentioned in our discussion of randomness in behavior a frequent pattern
 in development is what Kline coined as 
\begin_inset Quotes eld
\end_inset

0, 1, Rand, AI
\begin_inset Quotes erd
\end_inset

.
 According to this pattern we believe that GOAP or similar planning approaches
 are a promising option for current and near future NPC characters as they
 allow the replacement of randomness with artificial intelligence.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lit"
options "aaai"

\end_inset


\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Standard
World state symbols: 
\end_layout

\begin_layout Itemize
PlayerIsVisible
\end_layout

\begin_layout Itemize
HasSuppressionAmmunition 
\end_layout

\begin_layout Itemize
IsTargetDead
\end_layout

\begin_layout Itemize
ShockGunAmmunition 
\end_layout

\begin_layout Itemize
HasLowHealth 
\end_layout

\begin_layout Itemize
HasGrenadeAmmunition 
\end_layout

\begin_layout Itemize
HasRocketAmmunition
\end_layout

\begin_layout Itemize
HasLightiningGunAmmo
\end_layout

\begin_layout Itemize
HasMachineGunAmmo
\end_layout

\begin_layout Itemize
HasFlakAmmo
\end_layout

\begin_layout Itemize
HasAdrenaline
\end_layout

\begin_layout Itemize
PerformAdrenalineAction
\end_layout

\begin_layout Itemize
SuppressionFire
\end_layout

\begin_layout Itemize
HasGunAmmunition
\end_layout

\end_body
\end_document
